using Random
using StatsPlots
using Statistics
using LinearAlgebra
using DifferentialEquations
using Turing
using PyPlot
Random.seed!(0x7777)

k0 = [0.04, 3e7, 1e4]

# using Zygote, SciMLSensitivity
# setadbackend(:zygote)

# the ODE function
function robertson(dy, y, p, t)
    k = k0 .* p
    dy[1] = -k[1]*y[1]+k[3]*y[2]*y[3]
    dy[2] =  k[1]*y[1]-k[2]*y[2]^2-k[3]*y[2]*y[3]
    dy[3] =  k[2]*y[2]^2
end

# settings
Nsamples_k = 10000 # samples of true PDF
Nsamples_y = 300   # samples of plotted y profile
Nsamples_t = 1000  # samples of training data
y0 = [1.0, 0.0, 0.0]
p = [1., 1., 1.]
datasize = 20
tsteps = 10 .^ range(log10(1e-4), log10(1e8), length=datasize);
tspan = (0.0, tsteps[end]+1e-3);
solver = KenCarp4()
prob = ODEProblem(robertson, y0, tspan, p)

# generate data
y_true = solve(prob, solver, p=p, saveat=tsteps)
# y_scale = vec(maximum(y_true, dims=2));
y_scale = [1, 5e-5, 1]

## ================================
# data are generated by ground truth PDFs
@model function true_PDF()
    k1 ~ truncated(LogNormal(log(1.25), 0.10), 0.1, 2.)
    k2 ~ truncated(LogNormal(log(1.00), 0.15), 0.1, 2.)
    k3 ~ truncated(LogNormal(log(0.75), 0.20), 0.1, 2.)
end

chain_k = sample(true_PDF(), NUTS(0.65), Nsamples_k)
idxs = rand(1:Nsamples_k, Nsamples_y)
true_k_samples = chain_k[[:k1, :k2, :k3]][idxs,:,1]
true_y_samples = []

fig, axs = PyPlot.subplots(3, 2, figsize=(8,5))
fig.subplots_adjust(left=0.08, right=0.95, hspace=0.35, top=0.95)
for i in 1:3
    # axs[i,1].plot(tsteps, y_true[i,:], "-",  label="")
    axs[i,1].set_xscale("log")
    axs[i,1].set_ylabel("\$y_{$i}\$")
end
axs[2,1].legend(loc="best", frameon=false)
axs[3,1].set_xlabel("Time [s]")
for p in eachrow(Array(true_k_samples))
    y_sample = solve(prob, solver; p=p, saveat=tsteps)
    for i in 1:3
        axs[i,1].plot(tsteps, y_sample[i,:], alpha=0.01, color="b")
    end
    push!(true_y_samples, Array(y_sample))
end
display(fig)

# generate training data
y_data = deepcopy(y_true)
y_err = deepcopy(y_true)
for i in 1:datasize
    for j in 1:3
        idx = rand(1:Nsamples_y, Nsamples_y>>1)
        y_data[j,i] = mean([y_sample[j,i] for y_sample in true_y_samples[idx]]) #+ 0.01 * (rand()-0.5) * y_scale[j]
        y_err[j,i] = sqrt(var([y_sample[j,i] for y_sample in true_y_samples[idx]])) #+ var(0.01*rand(1000)*y_scale[j]) )
    end
end
for i in 1:3
    axs[i,1].errorbar(tsteps, y_data[i,:], yerr=2*y_err[i,:], fmt="k.", capsize=2, fillstyle="none", label="Data")
end
display(fig)


## ================================
# Prepare init PDF
@model function init_PDF()
    k1 ~ Uniform(0.2, 1.8)
    k2 ~ Uniform(0.2, 1.8)
    k3 ~ Uniform(0.2, 1.8)
end
chain_init = sample(init_PDF(), NUTS(0.65), Nsamples_k)
idxs = rand(1:Nsamples_k, Nsamples_y)
prior_samples = chain_init[[:k1, :k2, :k3]][idxs,:,1]

for p in eachrow(Array(prior_samples))
    y_pred = solve(prob, solver; p=p, saveat=tsteps)
    for i in 1:3
        axs[i,1].plot(tsteps, y_pred[i,:], alpha=0.01, color="k")
    end
end
axs[2,1].plot([], [], alpha=0.5, color="k", label="Ground truth")
axs[2,1].plot([], [], alpha=0.5, color="b", label="Prior")
axs[2,1].legend()
display(fig)

for i in 1:3
    axs[i,2].hist(chain_k[[:k1, :k2, :k3]][:,i,1], color="k", density=true, bins=50, alpha=0.4, label="Ground truth")
    axs[i,2].hist(chain_init[[:k1, :k2, :k3]][:,i,1], color="b", density=true, bins=50, alpha=0.4, label="Prior")
    axs[i,2].set_ylabel("PDF\$(\\theta_{$i})\$")
end
axs[2,2].legend()
axs[3,2].set_xlabel("\$\\theta\$")
display(fig)
fig.savefig("BayesRoberErrorbar_InitialState.png")

## ================================
# Prepare for optimization
@model function fitlv(data, prob)
    # Prior distributions.
    k1 ~ Uniform(0.2, 1.8)
    k2 ~ Uniform(0.2, 1.8)
    k3 ~ Uniform(0.2, 1.8)

    # Simulate ODE
    p = [k1, k2, k3]
    predicted = solve(prob, solver; p=p, saveat=tsteps)

    # Observations
    for i in 1:length(predicted)
        data[:, i] ~ MvNormal(predicted[i], diagm(9*y_err[i].*y_err[i]))
    end
    return nothing
end

model = fitlv(y_data, prob)
chain = sample(model, NUTS(0.65), MCMCSerial(), Nsamples_t, 1; progress=false)
Plots.plot(chain)

idxs = rand(1:Nsamples_t,Nsamples_y)
posterior_samples = chain[[:k1, :k2, :k3]][idxs,:,1]

# Plot results
fig, axs = PyPlot.subplots(3, 2, figsize=(8,5))
fig.subplots_adjust(left=0.08, right=0.95, hspace=0.35, top=0.95)
for i in 1:3
    axs[i,1].errorbar(tsteps, y_data[i,:], yerr=2*y_err[i,:], fmt="k.", capsize=2, fillstyle="none", label="Data \$\\pm 2\\sigma\$")
    axs[i,1].set_xscale("log")
    axs[i,1].set_ylabel("\$y_{$i}\$")
end
axs[3,1].set_xlabel("Time [s]")
axs[2,1].legend()
for (j,p) in enumerate(eachrow(Array(prior_samples)))
    y_pred = solve(prob, solver; p=p, saveat=tsteps)
    for i in 1:3
        axs[i,1].plot(tsteps, y_pred[i,:], alpha=0.02, color="b")
    end
end
for (j,p) in enumerate(eachrow(Array(posterior_samples)))
    y_pred = solve(prob, solver; p=p, saveat=tsteps)
    for i in 1:3
        axs[i,1].plot(tsteps, y_pred[i,:], alpha=0.01, color="r")
    end
end
axs[2,1].plot([], [], alpha=0.5, color="b", label="Prior")
axs[2,1].plot([], [], alpha=0.5, color="r", label="Posterior")
axs[2,1].legend()

for i in 1:3
    axs[i,2].hist(chain_k[[:k1, :k2, :k3]][:,i,1], color="k", density=true, bins=50, alpha=0.4, label="Ground truth")
    axs[i,2].hist(chain_init[[:k1, :k2, :k3]][:,i,1], color="b", density=true, bins=50, alpha=0.4, label="Prior")
    axs[i,2].hist(chain[[:k1, :k2, :k3]][:,i,1], color="r", density=true, bins=50, alpha=0.4, label="Posterior")
    axs[i,2].set_ylabel("PDF\$(\\theta_{$i})\$")
end
axs[2,2].legend()
axs[3,2].set_xlabel("\$\\theta\$")
display(fig)

fig.savefig("BayesRoberErrorbar_Optimized.png")
